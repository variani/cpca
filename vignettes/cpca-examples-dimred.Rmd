---
title: "Examples on using CPCA as a dimensionality reduction technique"
author: "Andrey Ziyatdinov"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Examples on using CPCA as a dimensionality reduction technique}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# About

## Include cpca package

```{r inc_cpca, cache = FALSE}
library(cpca)
```

## Include other packages

```{r inc, cache = FALSE, message = FALSE}
library(plyr)
library(ggplot2)
library(gridExtra)

library(pander)

library(pls)
library(MASS)

library(mclust)
library(ElemStatLearn)
```

Settings:

```{r settings, cache = FALSE, echo = T}
theme_set(theme_linedraw())
panderOptions('table.split.table', Inf)
panderOptions('knitr.auto.asis', FALSE)
```

# Examples

## Dataset iris

Loading the dataset:

```{r load_iris}
data(iris)
```

Preparing inputs to the CPCA model:

```{r prepare_iris}
X1 <- as.matrix(iris[, -5])
Y1 <- iris[, 5]
```

Running the CPCA model:

```{r iris_model}
m1 <- comprcomp(X1, Y1) 
```

Plotting scores:

```{r iris_varplot, fig.width = 9, fig.height = 3}
varplot(m1, X1, Y1)
```

```{r iris_scoreplot, fig.width = 6, fig.height = 4}
scoreplot(m1, X1, Y1)
```

## Bank notes dataset

Flury studied an interesting dataset known as the Swiss banknotes dataset,
which is available from an R package `mclust`.
The dataset consists of six measurements made on 200 banknotes,
where 100 are genuine and 100 are forged. 
The variables are: 

* width of the banknote; 
* height of the left-hand side of the banknote; 
* height of the right-hand side of the banknote; 
* distance between the top of the inner box to the upper border; 
* distance between the bottom of the inner box to the lower border; 
* diagonal of the inner box.


Loading the dataset:

```{r load_banknote}
data(banknote, package = "mclust")
```

```{r head_banknote, results = "asis"}
pander(head(banknote))
```

### Using all 6 variables

Preparing inputs to the CPCA model:

```{r prepare_banknote}
X2 <- as.matrix(banknote[, -1])
Y2 <- banknote[, 1]
```

Running the CPCA model:

```{r banknote_model}
m2 <- comprcomp(X2, Y2) 
```

Plotting scores:

```{r banknote_varplot, fig.width = 6, fig.height = 3}
varplot(m2, X2, Y2)
```

```{r banknote_scoreplot, fig.width = 6, fig.height = 4}
scoreplot(m2, X2, Y2)
```

The second component (CPC2, y-axis) on `scoreplot` captures little variance (5.7\%).
Let us see proportions of captured variance across all CPCs (expressed in percentage).

```{r banknote_com_var, results = "asis"}
pander(compvar(m2, X2, Y2, perc = TRUE, sorted = TRUE))
```

```{r banknote_com_var_gr, results = "asis"}
pander(compvar(m2, X2, Y2, grouping = TRUE, perc = TRUE))
```

#### Looking higher-order components

```{r banknote_scoreplot_23, fig.width = 6, fig.height = 4}
scoreplot(m2, X2, Y2, comp = c(2, 3))
```

```{r banknote_scoreplot_34, fig.width = 6, fig.height = 4}
scoreplot(m2, X2, Y2, comp = c(3, 4))
```

#### Scoring the components

```{r banknote_com_score, results = "asis"}
pander(compscore(m2, X2, sorted = TRUE))
```

```{r banknote_scoreplot_43, fig.width = 6, fig.height = 4}
scoreplot(m2, X2, Y2, comp = c(4, 3))
```


#### PCA

```{r bankbote_pca, fig.width = 6, fig.height = 4}
m21 <- prcomp(X2)

scoreplot(m21, Y = Y2)
```

### Using 4/6 variables

Preparing inputs to the CPCA model:

```{r prepare_banknote_4var}
X3 <- as.matrix(banknote[, 3:6])
Y3 <- banknote[, 1]
```

Running the CPCA model:

```{r banknote_model_4var}
m3 <- comprcomp(X3, Y3) 
```

Exploring captured variance from two perspectives:

```{r banknote_com_var_m3, results = "asis"}
pander(compvar(m3, X3, Y3, perc = TRUE))
```

```{r banknote_com_var_gr_m3, results = "asis"}
pander(compvar(m3, X3, Y3, grouping = TRUE, perc = TRUE))
```

Plotting scores:

```{r banknote_scoreplot_4var, fig.width = 6, fig.height = 4}
scoreplot(m3, X3, Y3)
```

PCA:

```{r bankbote_pca_4var, fig.width = 6, fig.height = 4}
m31 <- prcomp(X3)

scoreplot(m31, Y = Y3)
```

## Dataset vowel

[Data description](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/vowel/vowel.data)
at [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Vowel+Recognition+-+Deterding+Data)):

>The problem is specified by the accompanying data file, "vowel.data".  This
>consists of a three dimensional array: voweldata [speaker, vowel, input].
>The speakers are indexed by integers 0-89.  (Actually, there are fifteen
>individual speakers, each saying each vowel six times.)  The vowels are
>indexed by integers 0-10.  For each utterance, there are ten floating-point
>input values, with array indices 0-9.

>The problem is to train the network as well as possible using only on data
>from "speakers" 0-47, and then to test the network on speakers 48-89,
>reporting the number of correct classifications in the test set.

The data set `vowel.train`, as well as `vowel.test`,
is [available](http://www.inside-r.org/packages/cran/ElemStatLearn/docs/vowel.train)
within the R package `ElemStatLearn`.

Loading the dataset:

```{r load_vowel}
data(vowel.train, package = "ElemStatLearn")
vowel <- vowel.train

words <- c("heed", "hid", "head", "had", "hard", "hud", "hod", "hoard", "hood",
  "whod", "heard")
vowels <- c("hid", "hId", "hEd", "hAd", "hYd", "had", "hOd", "hod", "hUd", 
  "hud", "hed")

vowel <- within(vowel, {
  y <- factor(y, levels = 1:11, labels = vowels)
})
```

```{r vowel_tab_y, results = "asis"}
pander(table(vowel$y))
```

```{r head_vowel, results = "asis"}
pander(head(vowel))
```

Preparing inputs to the CPCA model:

```{r prepare_vowel}
X4 <- as.matrix(vowel[, -1])
Y4 <- vowel[, 1]
```

Running the CPCA model:

```{r vowel_model}
m4 <- comprcomp(X4, Y4) 
```

Plotting scores:

```{r vowel_varplot, fig.width = 9, fig.height = 8}
varplot(m4, X4, Y4)
```

```{r vowel_scoreplot, fig.width = 6, fig.height = 4}
scoreplot(m4, X4, Y4)
```

## Subset of dataset vowel

```{r prepare_subvowel}
subvowel <- subset(vowel, y %in% c("hid", "hId", "hOd", "hod"))
X5 <- as.matrix(subvowel[, -1])
Y5 <- subvowel[, 1]

Y5 <- droplevels(Y5)
```

Running the CPCA model:

```{r subvowel_model}
m5 <- comprcomp(X5, Y5) 
```

Plotting scores:

```{r subvowel_varplot, fig.width = 6, fig.height = 5}
varplot(m5, X5, Y5)
```

```{r subvowel_scoreplot, fig.width = 6, fig.height = 4}
scoreplot(m5, X5, Y5)
```

Computing the discrimination scores of CPCs:

```{r subvowel_compscore, results = "asis"}
pander(compscore(m5, X5, sorted = TRUE))
```

Now we can say something about the components in terms of their discrimination properties.

* The first component CPC1 has the lowest score among the others. 
  CPC1 is likely to capture the common variance among the four classes. 
  The previous figure produced by `scoreplot` confirms this conclusion.
* The component CPC3 has the highest score, and it is substantially higher in comparison with the next components CPC5, CPC9, etc.

Ploting with `scoreplot` using two pairs _CPC3 vs. CPC5_ and _CPC3 vs. CPC9_ should 
clarify the role of each of the three (most dicriminative) components.

```{r subvowel_scoreplot_comparison, fig.width = 8, fig.height = 3}
grid.arrange(
  scoreplot(m5, X5, Y5, comp = c(3, 5)),
  scoreplot(m5, X5, Y5, comp = c(3, 9)),
  nrow = 1)
```
  
It is clear that CPC3 performs reasonably well the main job on separation between two groups:
_hid_ / _hId_ and _hOd_ / _hod_ pairs of classes.
The further fine-grained job on separation e.g., between _hid_ and _hId_, is more comlicated.
The CPCA technique can offer to the user a solution of low quality:
(1) CPC5 for separation between _hid_ and _hId_ (the left panel of the figure above), and
(2) CPC9 for separation betwen _hOd_ and _hod_ (the right panel).


Interestingly, PCA applied to the same data has a similar group separation for all four classes,
but the first two PC1 and PC2 are not able to separate two _hid_ / _hId_ and _hOd_ / _hod_ groups linearly,
as CPCA did.

```{r subvowel_pca}
m51 <- prcomp(X5)
```

```{r subvowel_pca_plot, fig.width = 6, fig.height = 4}
scoreplot(m51, Y = Y5)
```

Let us close the loop and shows the results of LDA.

```{r subvowel_lda}
m52 <- lda(X5, Y5)
```

```{r subvowel_lda_plot, fig.width = 6, fig.height = 4}
scoreplot(m52, Y = Y5)
```

It seems that results of all CPCA, PCA and LDA are similar.
None of the techniques was able to cope with a cluster of red points (_hid_ class),
which tend to be closer to the _hOd_ / _hod_ group.

